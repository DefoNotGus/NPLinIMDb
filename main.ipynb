{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CM4107 Advanced Artificial Intelligence - Coursework\n",
    "## Comparative Evaluation of Two NLP Algorithms on Sentiment Classification Task\n",
    "Author: [Your Name]\n",
    "GitHub Repository: https://github.com/[your-repo-link]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 1: Dataset Description and Preprocessing\n",
    "The IMDB Dataset of Movie Reviews consists of 50,000 reviews with binary (positive/negative) sentiment labels.\n",
    "Reviews are divided into 25,000 for training and 25,000 for testing. Basic preprocessing such as tokenization\n",
    "and vectorization will be applied. We will work with the HuggingFace `datasets` library for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# Load Dataset\n",
    "dataset = load_dataset(\"imdb\")\n",
    "train_data = dataset[\"train\"]\n",
    "test_data = dataset[\"test\"]\n",
    "\n",
    "# Extract texts and labels\n",
    "train_texts, train_labels = train_data[\"text\"], train_data[\"label\"]\n",
    "test_texts, test_labels = test_data[\"text\"], test_data[\"label\"]\n",
    "\n",
    "# Preprocess text: vectorization\n",
    "# Using TF-IDF for Algorithm 1 (Naive Bayes) and Word Embeddings for Algorithm 2 (LSTM)\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000, stop_words=\"english\")\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(train_texts)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(test_texts)\n",
    "\n",
    "print(\"TF-IDF Vectorization Completed!\")\n",
    "print(f\"Training Feature Shape: {X_train_tfidf.shape}, Test Feature Shape: {X_test_tfidf.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2: Representation Learning\n",
    "Representation learning will use two forms:\n",
    "- TF-IDF Vectorization for traditional algorithms (Naive Bayes)\n",
    "- Token embeddings for neural methods (e.g., LSTM with GloVe embeddings)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Tokenization for Neural Networks\n",
    "tokenizer = Tokenizer(num_words=10000, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(train_texts)\n",
    "X_train_seq = tokenizer.texts_to_sequences(train_texts)\n",
    "X_test_seq = tokenizer.texts_to_sequences(test_texts)\n",
    "\n",
    "# Padding sequences\n",
    "max_length = 200  # Maximum review length\n",
    "X_train_padded = pad_sequences(X_train_seq, maxlen=max_length, padding=\"post\", truncating=\"post\")\n",
    "X_test_padded = pad_sequences(X_test_seq, maxlen=max_length, padding=\"post\", truncating=\"post\")\n",
    "\n",
    "print(f\"Padded Sequence Shape: {X_train_padded.shape}, {X_test_padded.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 3: Algorithms\n",
    "#### Algorithm 1: Naive Bayes Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Train Naive Bayes Classifier\n",
    "nb_model = MultinomialNB()\n",
    "nb_model.fit(X_train_tfidf, train_labels)\n",
    "\n",
    "# Evaluate Naive Bayes\n",
    "nb_predictions = nb_model.predict(X_test_tfidf)\n",
    "nb_accuracy = accuracy_score(test_labels, nb_predictions)\n",
    "nb_precision = precision_score(test_labels, nb_predictions)\n",
    "nb_recall = recall_score(test_labels, nb_predictions)\n",
    "nb_f1 = f1_score(test_labels, nb_predictions)\n",
    "\n",
    "print(\"Naive Bayes Performance:\")\n",
    "print(f\"Accuracy: {nb_accuracy}, Precision: {nb_precision}, Recall: {nb_recall}, F1-Score: {nb_f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Algorithm 2: Long Short-Term Memory (LSTM) Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "# LSTM Model Architecture\n",
    "vocab_size = 10000\n",
    "embedding_dim = 100\n",
    "lstm_units = 64\n",
    "\n",
    "lstm_model = Sequential([\n",
    "    Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length),\n",
    "    LSTM(lstm_units, return_sequences=False),\n",
    "    Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "lstm_model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Training LSTM Model\n",
    "lstm_model.fit(X_train_padded, train_labels, epochs=3, batch_size=64, validation_split=0.2)\n",
    "\n",
    "# Evaluate LSTM\n",
    "lstm_loss, lstm_accuracy = lstm_model.evaluate(X_test_padded, test_labels)\n",
    "print(f\"LSTM Loss: {lstm_loss}, LSTM Accuracy: {lstm_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 4: Evaluation and Comparison\n",
    "Performance Evaluation Metrics: Accuracy, Precision, Recall, F1-Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM Metrics\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "lstm_predictions = (lstm_model.predict(X_test_padded) > 0.5).astype(\"int32\").flatten()\n",
    "lstm_precision = precision_score(test_labels, lstm_predictions)\n",
    "lstm_recall = recall_score(test_labels, lstm_predictions)\n",
    "lstm_f1 = f1_score(test_labels, lstm_predictions)\n",
    "\n",
    "print(f\"LSTM Results:\\nAccuracy: {lstm_accuracy}\\nPrecision: {lstm_precision}\\nRecall: {lstm_recall}\\nF1: {lstm_f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of Results\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a bar chart comparing algorithm performance\n",
    "metrics = [\"Accuracy\", \"Precision\", \"Recall\", \"F1-Score\"]\n",
    "nb_scores = [nb_accuracy, nb_precision, nb_recall, nb_f1]\n",
    "lstm_scores = [lstm_accuracy, lstm_precision, lstm_recall, lstm_f1]\n",
    "\n",
    "x = range(len(metrics))\n",
    "plt.bar(x, nb_scores, width=0.4, label=\"Naive Bayes\")\n",
    "plt.bar([i + 0.4 for i in x], lstm_scores, width=0.4, label=\"LSTM\")\n",
    "plt.xticks([i + 0.2 for i in x], metrics)\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"Algorithm Performance Comparison\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
